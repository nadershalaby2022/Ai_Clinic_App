import pandas as pd
from pathlib import Path

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

import joblib


# ===================== Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª =====================

BASE_DIR = Path(__file__).resolve().parent
EXCEL_PATH = BASE_DIR / "clinic_data2.xlsx"      # Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ù†ÙØ³ Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ø¥ÙƒØ³Ù„
MODEL_PATH = BASE_DIR / "model_drug_reco.pkl"    # Ø¯Ù‡ Ø§Ù„Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø¨ÙŠØ­ØªØ§Ø¬Ù‡


def build_training_dataset():
    """
    Ù†Ø¨Ù†Ù‰ Ø¯Ø§ØªØ§ ØªØ¯Ø±ÙŠØ¨ Ù…Ù†:
    - Patient_Visits  (Ø§Ù„ØªØ´Ø®ÙŠØµ + Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ + Ø§Ù„Ø³Ù† + Ø§Ù„ÙˆØ²Ù†)
    - Visit_Drugs     (Ø§Ù„Ø£Ø¯ÙˆÙŠØ©)
    ÙƒÙ„ ØµÙ = (Diagnosis, Chief_Complaint, Age_Months, Weight_KG) â†’ Drug_Name
    """
    print(f"ğŸ“‚ Loading Excel file: {EXCEL_PATH}")
    xls = pd.ExcelFile(EXCEL_PATH)

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø´ÙŠØªØ§Øª
    df_visits = pd.read_excel(xls, "Patient_Visits")
    df_drugs  = pd.read_excel(xls, "Visit_Drugs")

    # Ù†Ø®ØªØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù„Ù‰ Ù…Ø­ØªØ§Ø¬ÙŠÙ†Ù‡Ø§ Ù…Ù† Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª
    visits_cols = [
        "Visit_ID",
        "Diagnosis",
        "Chief_Complaint",
        "Age_Months",
        "Weight_KG",
    ]
    df_visits_small = df_visits[visits_cols]

    # Ù†Ø¹Ù…Ù„ join Ø¹Ù„Ù‰ Visit_ID Ø¹Ù„Ø´Ø§Ù† Ù†Ø¹Ø±Ù ÙƒÙ„ Ø¯ÙˆØ§Ø¡ ÙƒØ§Ù† ØªØ§Ø¨Ø¹ Ù„Ø£Ù‰ ØªØ´Ø®ÙŠØµ/Ø£Ø¹Ø±Ø§Ø¶
    df = df_drugs.merge(df_visits_small, on="Visit_ID", how="inner")

    # Ù†Ø´ÙŠÙ„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„Ù‰ Ù†Ø§Ù‚ØµØ©
    df = df.dropna(
        subset=["Drug_Name", "Diagnosis", "Chief_Complaint", "Age_Months", "Weight_KG"]
    )

    print(f"âœ… Training rows after merge & dropna: {len(df)}")

    return df


def train_and_save_model():
    df = build_training_dataset()

    # ========== Features & Target ==========
    target_col = "Drug_Name"

    feature_cols = ["Diagnosis", "Chief_Complaint", "Age_Months", "Weight_KG"]

    X = df[feature_cols]
    y = df[target_col]

    cat_cols = ["Diagnosis", "Chief_Complaint"]
    num_cols = ["Age_Months", "Weight_KG"]

    # ========== Preprocessing ==========
    preprocessor = ColumnTransformer(
        transformers=[
            ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
            ("num", StandardScaler(), num_cols),
        ]
    )

    # ========== Model ==========
    clf = RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        class_weight="balanced_subsample",
    )

    model = Pipeline(
        steps=[
            ("preprocess", preprocessor),
            ("clf", clf),
        ]
    )

    # ========== Train/Test Split ==========
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.2,
        random_state=42,
        stratify=y,
    )

    print("ğŸš€ Training model...")
    model.fit(X_train, y_train)

    train_score = model.score(X_train, y_train)
    test_score  = model.score(X_test, y_test)

    print(f"âœ… Train accuracy: {train_score:.3f}")
    print(f"âœ… Test  accuracy: {test_score:.3f}")

    # ========== Save ==========
    joblib.dump(model, MODEL_PATH)
    print(f"ğŸ’¾ Model saved to: {MODEL_PATH}")


if __name__ == "__main__":
    train_and_save_model()
